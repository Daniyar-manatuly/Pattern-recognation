import sys
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from PyQt6.QtWidgets import QApplication, QLabel, QPushButton, QVBoxLayout, QWidget
from PyQt6.QtGui import QImage, QPixmap, QPalette, QBrush
from PyQt6.QtCore import QTimer


data = pd.read_csv(r'C:\Users\opste\Downloads\fer2013.csv')


X = np.array([np.array(row.split(), dtype='uint8').reshape(48, 48, 1) for row in data['pixels']])
y = to_categorical(data['emotion'], num_classes=7)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


model = Sequential([
    Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')
])


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))


test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Точность модели: {test_acc * 100:.2f}%')


model.save("emotion_cnn.keras")


EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprised']
COLORS = {'Angry': (0, 0, 255), 'Disgust': (0, 255, 0), 'Fear': (255, 0, 255),
          'Happy': (0, 255, 255), 'Neutral': (255, 255, 255), 'Sad': (255, 0, 0), 'Surprised': (0, 165, 255)}


class EmotionApp(QWidget):
    def __init__(self):
        super().__init__()

        
        self.setWindowTitle("Emotion Detection")
        self.setGeometry(100, 100, 800, 600)

       
        background = QPixmap(r"C:\Users\opste\Downloads\interfeis.jpg")
        palette = self.palette()
        palette.setBrush(QPalette.ColorRole.Window, QBrush(background))
        self.setPalette(palette)

        self.video_label = QLabel(self)
        self.start_button = QPushButton("Start detection")
        self.exit_button = QPushButton("Closed")

        
        layout = QVBoxLayout()
        layout.addWidget(self.video_label)
        layout.addWidget(self.start_button)
        layout.addWidget(self.exit_button)
        self.setLayout(layout)

        
        self.start_button.clicked.connect(self.start_camera)
        self.exit_button.clicked.connect(self.close)

        
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.cap = None

        
        self.model = load_model("emotion_cnn.keras")
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    def start_camera(self):
       
        self.cap = cv2.VideoCapture(0)
        self.timer.start(30)

    def update_frame(self):
        
        ret, frame = self.cap.read()
        if not ret:
            return

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

        for (x, y, w, h) in faces:
            face = gray[y:y+h, x:x+w]
            face = cv2.resize(face, (48, 48))
            face = face.reshape(1, 48, 48, 1) / 255.0

            prediction = self.model.predict(face)
            emotion = EMOTIONS[np.argmax(prediction)]
            color = COLORS[emotion]

            
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)

       
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        height, width, channel = frame.shape
        bytes_per_line = channel * width
        qt_image = QImage(frame.data, width, height, bytes_per_line, QImage.Format.Format_RGB888)
        self.video_label.setPixmap(QPixmap.fromImage(qt_image))

    def closeEvent(self, event):
        
        self.timer.stop()
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        event.accept()

app = QApplication(sys.argv)
window = EmotionApp()
window.show()
sys.exit(app.exec())
